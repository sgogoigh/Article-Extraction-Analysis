Data integration and big data performance using Elasticsearch

A Leading Tech Firm in the USA


Software, Business Solutions, Consulting
Migrate existing databases from Postgres to elastic search since Elasticserach performs better in search operations. In addition to this, all of the backend javascript also needed to be changed in order to query the new elasticsearch database.
The client’s website was a visualization tool. It also had GUI to add filters. To make the visualizations, at least 50,000 records needed to be pulled from the Postgres database whose size would be around 200mbs. This would take a lot of time (nearly 20-30 secs). Adding filters would take additional time. So our task was to move the entire database over to Elasticsearch from postgres since it is way more faster in search operations and also filtering data. Since the database was changed, we also had to write new backend code that would now query the Elasticsearch database.
Domain-Specific Language for elasticsearch


Elasticsearch query knowledge
Postgres query knowledge


Amazon Web Services (AWS)
To solve the above mentioned problem, we used gzip in the request url’s header. This significantly reduced the execution times.
Earlier postgres infrastructure which took around 20-30 secs now too consistently less than 10 secs to perform filter and search operations. This would contribute to a better user experience.
From Complexity to Clarity: Transforming Data into Decisions through Mixed Modelling
AWS CodePipeline is utilized for automatically building and deploying Lambda functions in AWS
Dockerize the AWS Lambda for serverless architecture